project: https://www.youtube.com/playlist?list=PLy_6D98if3ULEtXtNSY_2qN21VCKgoQAE

https://dev.to/techschoolguru/how-to-setup-github-actions-for-go-postgres-to-run-automated-tests-81o

// create DB schema 
https://dbdiagram.io

// TablePlus to manage postgres

// Database migration library
https://github.com/golang-migrate/migrate
> brew install golang-migrate
> migrate -version // v4.15.2
> create ./db/migration folders
// create migration
> migrate create -ext sql -dir db/migration -seq <migration_name> // migration_name
// -seq = sequential version number

// create db from inside the shell 
> createdb --username=root --owner=root simple_bank
> dropdb simple_bank

// manage Postgre container from outside the shell using docker command
> docker exec -it postgres12 createdb --username=root --owner=root simple_bank
> docker exec -it postgres12 dropdb simple_bank

// Docker Network
docker container inspect [docker-id] // check the network setting of the container
docker network ls // show existing docker networks
docker network inspect bridge // inspect the 'bridge' network param for existing Containers

// Create Network for containers
docker network create [network_name] // create new docker network
docker network connect [network_name] [container_name] // connect existing container to a network

// If two containers are on the same network, they can connect to eachother by Container-Name

// run migration from project folder
> migrate -path ../db/migration -database "postgresql://root:secret@localhost:5432/simple_bank?sslmode=disable" -verbose up
> migrate -path ../db/migration -database "postgresql://root:secret@localhost:5432/simple_bank?sslmode=disable" -verbose down

// create new migration version
> migrate create -ext sql -dir db/migration -seq add_users

// Auto generating database CRUD code using SQLC
https://github.com/kyleconroy/sqlc
// Installation
> brew install sqlc
> sqlc init

// Creating tests for CRUD DB functinality
// create main_test.go to hold all the db connection and query
// create account_test.go for testing functionality
// Install a PG drive from https://github.com/lib/pq using "go get github.com/lib/pq"

// Use Testify package for checking the test results
// https://github.com/stretchr/testify
// go get github.com/stretchr/testify
// import "github.com/stretchr/testify/require"

// CI using "Github Action" with Go + postgres
// to auto build, test and deploy
// Define a Workflow built from several Jobs
// The Workflow can be triggered by one of 3 (Change to repository on Github, Schedule , or button click)
// Creating a Workflow in .github/workflows/ci.yml file on the repository main folder
// Jobs are defined by a Runner (server that listen for available jobs)
// The Runner will execute the job and push the report to a log hosted by Github
// A Job is a set of steps that will be executed inside a Runner
// Steps are actions inside a Job and run serially 
// A Step can contain several actions
// Action is single command

// Web Server : Gin

// Viper
// Read config from file or ENV variables
// Repo : https://github.com/spf13/viper
// Install : go get github.com/spf13/viper
// File - config for local developement and testing
// Ev. variables - config for docker production
// Usage : DB configuration in main_test.go and main.go files 
// Create a file in root 'app.env' to store config values for developement
// copy the db variables from main.go
// configure Viper to load the app.env file -> create a file config.go in ./utill
// Init the LoadConfig in main.go

// Http testing using Mock-DB
// https://github.com/golang/mock
// Install : go install github.com/golang/mock/mockgen@v1.6.0
>  ls -l ~/go/bin
>  which mockgen -> result with 'mockgen not found' becouse 
// create  new folder in db/mock
// check 'mockgen help' and choose 'Reflect mode'
// mockgen [package-name] [path-to-generated-output] [path-to-store-interface] [interface-name] 
> mockgen -package mockdb -destination db/mock/store.go github.com/liorlavon/simplebank/db/sqlc Store 
// find the generated file in db/mock/store.go
Now we can start writing tests for the API
// create a account_test.go in the api folder

Custom Validator: (for Currency options)
vaidate input Params
create 'validator.go file 
import "github.com/go-playground/validator/v10"
var validCurrency validator.Func 
create a new file in util package called 'currency.go' to validate one currency filed with a list of possible options
> Next: register the custom validator with Gin in server.go file
	// get currect validator engine(interface) and conver it to *validator.Validate pointer
	v, ok := binding.Validator.Engine().(*validator.Validate)
	if ok {
		// register new validator
		v.RegisterValidation("currency", validCurrency)
	}
> Next: change the binding validation input for currenct
in account.go change oneof=USD EUR with currency
    //Currency string `json:"currency" binding:"required,oneof=USD EUR"`
    Currency string `json:"currency" binding:"required,currency"`

// Password storing in the DB
// using BCRYPT algorythem (Cost & Salt)
// checkout password.go 

// Chapter 18:
// Implement gomock Custom matcher for unit-test
// see example: eqCreateUserParamMatcher in user_test.go

// Token based authentication 
// JWT vs PASETO
// JWT github : https://github.com/dgrijalva/jwt-go
// Install : go get github.com/dgrijalva/jwt-go
// PASETO github : https://github.com/o1egl/paseto
// Install : go get github.com/o1egl/paseto

// Authentication Middelware
// Add the token to the Authorization header of the request to successs authentication and return the relevant details of the token owner
// implement gin Authentication Middelware
// A Middelware grab a request to a specific route, Validate the request and can Abort() or call Next() to pass the request to the next Middleware or the handler
// Add the authMiddleware to the server.go in the setupRoute function after each route that needs authentication

// Authorization defines what permissions 'user' has access to

// AES
// Git Hub Action to automaticly Build & Push docker images to ECR (Elastic Conrainer Registery)
// Elastic Conrainer Registery is a managed docker container registery to store deploy and manage docker container images
- In AWS create repository to store images
Push code to Github - Build - Test - Docker - store in AWS

// RDS on AWS
// Set Postgres/RDS instance on AWS
// use Make aws_migrateup to populate the tables

// Secrets manager on AWS
// We need to replace the values in app.env with real production values
// the values will be stored in Secrets vault
// Using the github/Action deploy.yml , we will insert the values from AWS Secrets
// So when we run the docker on AWS it will have all the production values
// We will store the productionvalues in Secret Manager on aws
// use OpenSSL generate 32 charecter secret
openssl rand -hex 64 | head -c 32 // generate 64 byte string with only hex characters and take only the first 32 characters 

aws secretsmanager describe-secret --region eu-central-1 --secret-id [NAME / ARN] 
aws secretsmanager get-secret-value --region eu-central-1 --secret-id [NAME / ARN] --query SecretString --output text
// Now we need to store this result into the app.env file 
// we will use ./jq app to convert to json
// to verify installation run : js --version

aws secretsmanager get-secret-value --region eu-central-1 --secret-id simple_bank --query SecretString --output text | jq 'to_entries'
Result:
[
  {
    "key": "DB_SOURCE",
    "value": "postgresql://root:RRjZZB39FeX7XRwXCYyn@simple-bank.cagrmci8jrd8.eu-central-1.rds.amazonaws.com:5432/simple_bank"
  },
  {
    "key": "DB_DRIVER",
    "value": "postgres"
  },
  {
    "key": "SERVER_ADDRESS",
    "value": "0.0.0.0:8080"
  },
  {
    "key": "ACCESS_TOKEN_DURATION",
    "value": "15m"
  },
  {
    "key": "TOKEN_SYMMETRIC_KEY",
    "value": "37672e772d7fa78e641f11a6c59078c5"
  }
]


// Now we need to itterate on each one and transform them into Key/Value pairs in app.env
// Get both Key/Value and format the output to be similar to app.env format
aws secretsmanager get-secret-value --region eu-central-1 --secret-id simple_bank --query SecretString --output text | jq -r 'to_entries|map("\(.key)=\(.value)")|.[]'
Result :
DB_SOURCE=postgresql://root:RRjZZB39FeX7XRwXCYyn@simple-bank.cagrmci8jrd8.eu-central-1.rds.amazonaws.com:5432/simple_bank
DB_DRIVER=postgres
SERVER_ADDRESS=0.0.0.0:8080
ACCESS_TOKEN_DURATION=15m
TOKEN_SYMMETRIC_KEY=37672e772d7fa78e641f11a6c59078c5

// Put this command into the deploy.yml to overide the app.env file before pushing the docker image
aws secretsmanager get-secret-value --region eu-central-1 --secret-id simple_bank --query SecretString --output text | jq -r 'to_entries|map("\(.key)=\(.value)")|.[]' > app.env

// after a new image was pushed to ECR, we want to check that the image connects to AWS RDS and use the production env as planned
// Download the image to 'local' by 'copy uri' and run "docker pull [uri]" in the terminal
// the ECR repo is private, so we need to login to ECR before pulling the image

// Login to ECR
aws ecr get-login-password \
    --region <region> \
| docker login \
    --username AWS \
    --password-stdin <aws_account_id>.dkr.ecr.<region>.amazonaws.com

aws ecr get-login-password --region eu-central-1 | docker login --username AWS --password-stdin 354885450543.dkr.ecr.eu-central-1.amazonaws.com    
// replace <region> and <aws_account_id> with account info

// The app.env is loaded when the app starts, but the $DB_SOURCE is alos used when the container boots and run migrate but the $DB_SOURCE is empty
// from the terminal , echo $DB_SOURCE is empty
// Therfore we need to load the variables from app.env to the current shell as enviroment variables
// call: 'source app.env' to load the variables to the shell as enviroment variables
// now calling 'echo $DB_SOURCE' echo the value from env variables.
// Copy the 'source app.env' command to the start.sh file before we call db-migrate command and adjust its location to: /app/app.env


// AWS EKS 
// Elastic Kubernetes Service,  Container orchestration engin
// Fully managed Kubernetes control plane

// kubectl
// Connect to EKS using 'kubectl' & 'k9s' tools
// Install kubectl: https://kubernetes.io/docs/tasks/tools/
// https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/#install-with-homebrew-on-macos
// Watch chepter 31

// Cubectl Commands
kubectl version --client // print version
kubectl version --client --output=json
kubectl cluster-info                      // verify & connect to the aws EKS cluster
// config is stored in '~/.kube/config'

// fetch the aws EKS cluster information and store result in '~/.kube/config'
aws eks update-kubeconfig --name [simple-bank] --region [region]

// In case on multiple eks clusters in ~/.kube/config ,we can switch to the relevant context with :
kubectl config use-context [arn:... of cluster]

// switch cli-user between profiles, and give cli user access to kubectl
export AWS_PROFILE=github
export AWS_PROFILE=default

kubectl get service 
kubectl get pods



// Tool : k9s, https://k9scli.io
// Kubernetes CLI To Manage Your Clusters In Style!
// UI tool to manage the k8 cluster
// Install via: 'brew install derailed/k9s/k9s'
// Start type 'k9s'
// Commands
// Switch namespace => : ns 
// List ConfigMaps => : configmap
// List all services => : service
// List all pods => : pods
// List all krobjobs => : cj 
// List all nodes => : nodes
// List all ingress => : ingress
// <ctrl-d> Delete resource
// <d> describe resource
// :clusterissuer
// :secret  => to find the private-key of the issuer
// :certificates 
// :certificaterequests

// Deploy web app to K8 on aws
// Create a deployment // description of how we want our container to be deployed
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
// in the eks folder , create a file deployment.yaml
Deploy => 'kubectl apply -f eks/deployment.yaml'
// when the service is running, we still Cant connect to the pod becouse the IP is private
// So we need to deploy a Service => https://kubernetes.io/docs/concepts/services-networking/service/
// create a file eks/service.yaml and add the configuration
// to set the external IP: add 'type: LoadBalancer' in the eks/service.yaml config file
// to apply the service config by calling : 'kubectl apply -f eks/service.yaml'
// to check the external IP, run : 'nslookup [external_ip]'

// Register a domain
// now we have setup the k8s service to expose external IP , next we should connect it to a Domain registry using A record
// we can use aws service Route53 to register a domain, but liorlavon.me is registered in Godaddy so i will use it and route to a DNS on Route53


// Ingress / K8S
// setup single A record and define miltiple rules in the ingress/config file to route traffic to different services
// Ingress will also handle LoadBalancing and SSL termination, so supporting https is easy
// Ingress expose http/https from outside to a specific webservice based on rule-set
https://kubernetes.io/docs/concepts/services-networking/ingress/

- In 'eks/service.yaml' change the exposure of the service from LoadBalancer to ClusterIP to remove the external exposure
- create eks/ingress.yaml file
- include the ingressClass object in the ingress script

// Configuring external address to the ingress
// we need to setup the A record to route incoming traffic to the ingress, by deploying 'Ingress Controller'
https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/
// we will install the 'nginx ingress controllers', follow the link in the documentation

# Update Ingress to enable SSL / TLS / https 
# the login request of the cliet to the k8s/80 is over plain http connection, this is not good becouse the user credentials can be stolen in the middle of the request
# to secure the connection we will add Free TLS certificate for K8S
# we will use 'cert manager' to automaticly generate and renue the certificate
https://cert-manager.io
# ACME protocol : Autoated Certificate Managment Enviroment
# we will follow the HTTP-01 challange to create the certificate
# but if we have multiple services , implement the DNS-01 challange

# Implementation:
# from cert-manager site, install sert-manager addon for k8s cluster
https://cert-manager.io/docs/installation/kubectl/
# a new namespace 'cert-manager' is added to k8s , see :ns
# Verify installation: kubectl get pods --namespace cert-manager
# Next we will config & deploy a certificate issuer to the cluster
# we use ACME protocol, create a new file eks/issuer.yaml
# deploy the script to the k8s, in k9s check :clusterissuers 
# Result: 'The ACME account was registered with the ACME server'

# to find the cert-manager private-key of the ceritifcate, use :secret
# result : letsencrypt-account-private-key

# at this point both :certificates and :certificaterequests list are both empty
# becouse we havent attached the issuer to Ingress yet
# to attach the issuer to the Ingress , in Ingress.yaml, under the metadata section of the ingress add annotation section
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt     # refer to the cluster issuer name in issuer.yaml

# Github Action, auto Deploy docker images from ECS to ECR
# so far we created yaml files in /eks folder and deployed them manually
# to automate the process :
# install 'kubectl' in Github-Action
  # find 'Kubectl tool installer' in github marketplace
  # add the code snipit to .github/.../deploy.yml file
# in the deploy.yaml change the build process to add the tag latest on each new build
# in the deploy.yaml add a last step 'Deploy image to EKS' and invoke Kubectl actions from /eks folder


# Advanced Topics

# Refresh Token: used with longer time expiration, to refrash the main token
access_token is for 10 - 15 minutes
refresh_token is for 1 - 7 days
The refresh_token should be stored in the session table in the database, with is_blokked filed
and the ability to revoke the refresh_token if needed.

We want to add a refrash_token in the login response with a longer duration

# Create DB documentation and SQL
https://dbdocs.io/?utm_source=dbdiagram

Dependencies :
Install NodeJS & NPM

install dbdocs
> npm install -g dbdocs
https://dbdocs.io/liorlavon554

# dbml (Generating sql dump file)
https://dbml.dbdiagram.io/home/?utm_source=dbdocs
Install > npm install -g @dbml/cli

# Swagger
https://www.youtube.com/watch?v=AtaXj2hj074&t=290s
https://www.youtube.com/watch?v=AtaXj2hj074&t=10s 
go get -u github.com/swaggo/swag/cmd/swag
go get -u github.com/swaggo/files
go get -u github.com/swaggo/gin-swagger

# Setup background process using Asyncq 
# used to push the event asynchronically
https://github.com/hibiken/asynq
The client 'SimpleBack' is pushing tasks to the queue using tags, then the server pickup a task and instantiate a background process


# Redis
- from docker hub
docker pull redis:7-redis
# test redis
docker exec -it redis redis-cli ping






